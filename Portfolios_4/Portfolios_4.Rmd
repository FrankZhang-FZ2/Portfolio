---
title: "Portfolios_4"
author: "Qilin Zhang"
date: "2023-04-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
```

```{r pac, message = FALSE, include=FALSE}
library(summarytools)
library(tidyverse) #data wrangling
library(codebook) #codebook generation
library(future) #reliability
library(ufs) #reliability
library(GGally) #reliability
library(GPArotation) #reliability
library(rio) #reading in different file types
library(labelled) #labeling data
library(psych)
library(corrplot) 
library("psych")
library(mirt)
library(eRm)
library(mice)
library(lavaan)
library(semPlot)
library(parameters)
library(broom)
library(likert)
library(sjPlot)

#if need to download the epmr package
#if (!require("devtools")) install.packages("devtools")   
#devtools::install_github("talbano/epmr")
```

###cleaning

```{r cleaning code, include = FALSE}

## use other read functions as appropriate for file type

dict <- rio::import(file = "DGS_S1_dictionary.xlsx") #dictionary

data <- read.csv(file = 'DGS_S1_Deidentified For Analyses.csv', sep = ",") #data

##trim based on completion time (>180)
data <- data %>%
  filter(Duration..in.seconds. >= 180)

data <- data[-c(1:2),-c(1:7)]

## Variable types 
names <- dict %>% 
  filter(type == "character") %>% 
  pull(variable)
data[,names] <- 
  lapply(data[,names], as.character)

names <- dict %>% 
  filter(type == "factor") %>% 
  pull(variable)
data[,names] <- 
  lapply(data[,names], as.numeric) #factor variables are coded as numeric for codebook purposes

names <- dict %>% 
  filter(type == "numeric") %>% 
  pull(variable)
data[,names] <- 
  lapply(data[,names], as.numeric)

rm(names)

##data completion check and imputation

#remove failed attention check -55 participants from this process
data <- data %>%
  filter(DGS_31 == 4) %>%
  filter((DGS_53 == 2))

#prepare dataframe for different scales
DGS <- data[grep("DGS_1",colnames(data)):grep("DGS_75",colnames(data))] %>%
  select(!c(DGS_31,DGS_53))


##remove people that gave 90% of the same answer in DGS

for(i in 1:7){
  percent <- function(x){
  sum((x == i)/length(x)*100)
  }
  number <- apply(DGS,1,percent)
  data <- data[c(number<90),]
}

#function for checking percentage of missing data (unit=%)
percent_missing <- function(x){
  sum(is.na((x))/length(x)*100)
}
missing_R <- apply(data,1,percent_missing)
table(missing_R)

#use if don't want imputation(turn off if I need imputation)
replace_rows <- subset(data, missing_R<=0)
data <- replace_rows
```

```{r imputation, include= FALSE}
if(FALSE){
#subset based on filtering criteria
replace_rows <- subset(data, missing_R<=10)
no_rows <- subset(data, missing_R>10)

missing_C <- apply(replace_rows,2,percent_missing)
table(missing_C) #no concern here

replace_data <- replace_rows[,1:85]
leftout <- replace_rows[,86:91]

#check where the NAs are if needed
rindex <- rep(FALSE, nrow(replace_data))
for (i in 1:nrow(replace_data)){
  for (j in 1:grep("DGS_75",colnames(replace_data))){
    if( is.na(replace_data[i,j])){
      rindex[i] = TRUE
      j = ncol(replace_data)+1
    }
  }
}
data_error <- replace_data[rindex,]

rm(data_error)
#imputation
temp <- mice(replace_data)
fixed_data <- complete(temp)  #imputation using mice package
data <- cbind(fixed_data,leftout) #no additional participants were removed

rm(fixed_data,leftout,no_rows,replace_data,replace_rows)
}
```

```{r recode, include=FALSE}
##recode

#DGS recode
likert <- dict %>% 
  filter (value_label == "1 = Strongly disagree, 2 = Disagree, 3 = Somewhat disagree, 4 = Neither agree nor disagree, 5 = Somewhat agree, 6 = Agree, 7 = Strongly agree") %>%
  pull(variable)
add_likert <- function(x) {
  val_labels(x) <- c("Strongly disagree"= 1, "Disagree" = 2, "Somewhat disagree" = 3, "Neither agree nor disagree" = 4, "Somewhat agree" = 5, "Agree" = 6, "Strongly agree" = 7) 
  x
}
data <- data %>%
  mutate_at(likert, 
            add_likert)  

rm(likert, add_likert)

#HEXACO_C

likert <- dict %>% 
  filter (value_label == "1 = Strongly disagree, 2 = Somewhat disagree, 3 = Neither agree nor disagree, 4 = Somewhat agree, 5 = Strongly agree") %>%
  pull(variable)
add_likert <- function(x) {
  val_labels(x) <- c("Strongly disagree"= 1, "Somewhat disagree" = 2, "Neither agree nor disagree" = 3, "Somewhat agree" = 4, "Strongly agree" = 5) 
  x
}
data <- data %>%
  mutate_at(likert, 
            add_likert)  

rm(likert, add_likert)

## Reverse-scoring 
reversed_items <- dict %>%  #make a list of reversed items
 filter (keying == -1) %>% 
 pull(variable)

data <- data %>%  #reverse values in data
  mutate_at(reversed_items,
          reverse_labelled_values)

rm(reversed_items)

##scale construction

## Variable labels
var_label(data) <- dict %>% 
  select(variable, label) %>% 
  dict_to_list()

rm(extra)

##DGS
DGS <- dict %>% 
  filter (scale == "DGS") %>% 
  pull(variable)

data$DGS <- data %>% 
  select(all_of(DGS)) %>% 
  aggregate_and_document_scale()

##HEXACO_C

HEXACO_C <- dict %>% 
  filter (scale == "HEXACO_C") %>% 
  pull(variable)

data$HEXACO_C <- data %>% 
  select(all_of(HEXACO_C)) %>% 
  aggregate_and_document_scale()

###7 factors model
M7_1 <- dict %>% 
  filter (M7_concise == 1) %>% 
  pull(variable)
M7_2 <- dict %>% 
  filter (M7_concise == 2) %>% 
  pull(variable)
M7_3 <- dict %>% 
  filter (M7_concise == 3) %>% 
  pull(variable)
M7_4 <- dict %>% 
  filter (M7_concise == 4) %>% 
  pull(variable)
M7_5 <- dict %>% 
  filter (M7_concise == 5) %>% 
  pull(variable)
M7_6 <- dict %>% 
  filter (M7_concise == 6) %>% 
  pull(variable)
M7_7 <- dict %>% 
  filter (M7_concise == 7) %>% 
  pull(variable)

###7 factors model

data$M7_1 <- data %>% 
  select(all_of(M7_1)) %>% 
  aggregate_and_document_scale()

data$M7_2 <- data %>% 
  select(all_of(M7_2)) %>% 
  aggregate_and_document_scale()

data$M7_3 <- data %>% 
  select(all_of(M7_3)) %>% 
  aggregate_and_document_scale()

data$M7_4 <- data %>% 
  select(all_of(M7_4)) %>% 
  aggregate_and_document_scale()

data$M7_5 <- data %>% 
  select(all_of(M7_5)) %>% 
  aggregate_and_document_scale()

data$M7_6 <- data %>% 
  select(all_of(M7_6)) %>% 
  aggregate_and_document_scale()

data$M7_7 <- data %>% 
  select(all_of(M7_7)) %>% 
  aggregate_and_document_scale()
```

```{r preparation}

##rename
data_cleaned <- data

#prepare dataframe for different scales
DGS <- data_cleaned[grep("DGS_1",colnames(data_cleaned)):grep("DGS_75",colnames(data_cleaned))]
DGS <- DGS[,-c(grep("DGS_31",colnames(DGS)),grep("DGS_53",colnames(DGS)))]

HEXACO_C <- data_cleaned[grep("HEXACO_C_1",colnames(data_cleaned)):grep("HEXACO_C_10",colnames(data_cleaned))]
```


###EFA 

```{r EFA_all}
##descriptive analysis for all items
DGS_descriptive <- describe(DGS)
##EFA for DGS

#Kaiser Criterion
ev<- eigen(cor(DGS))
ev$values
sum(ev$values > 1)
sum(ev$values > .7)

#scree plot and parallel analysis
scree(DGS, pc=FALSE)
fa.parallel(DGS,
            fm="ml",
            fa="fa")

```

6 factors concise model

```{r 6_factors_concise}
# try to trim out behavioral items and create 6 factors
DGS_fit_6C <- DGS %>%
  select(c(
    DGS_4,DGS_12,DGS_15,DGS_17,DGS_18,DGS_19,DGS_22,DGS_30,DGS_32,DGS_33,DGS_34,DGS_36,DGS_49,DGS_56,DGS_57,DGS_58,DGS_60,DGS_61,DGS_63,DGS_74,DGS_75,
    DGS_5,DGS_10,DGS_26,DGS_41,DGS_54,DGS_69,DGS_72,
    DGS_9,DGS_20,DGS_27,DGS_29,DGS_46,DGS_52,
    DGS_2,DGS_11,DGS_28,DGS_35,DGS_48,DGS_68,
    DGS_1,DGS_21,DGS_44,DGS_65,
    DGS_6,DGS_38,DGS_39,DGS_45,DGS_67,DGS_70,DGS_71
  ))
# 64 59 47(poor conceptual fit) 55(mixed conceptual fit) 23(poor conceptual fit),37 poor fitting
# DGS_13,DGS_73,DGS_16,DGS_40,DGS_42,DGS_43 out due to low loadings (<0.3)
EFA_fit_6C <- fa(DGS_fit_6C,
             nfactors = 6,
             rotate = "oblimin",
             fm="ml")
EFA_fit_6C <- fa.sort(EFA_fit_6C)
print(EFA_fit_6C$loadings, cutoff = 0.3)
print(EFA_fit_6C$loadings)
fa.diagram(EFA_fit_6C,
            sort = T,
            rsize = 0.5,
           cex = 0.8,
           main = "visualization of 6 factors structure")

#initial communalities
smc(DGS_fit_6C)

#fit
EFA_fit_6C$rms  # Root mean square of the residuals (lower the better)
EFA_fit_6C$RMSEA # root mean squared error of approximation (lower the better)
EFA_fit_6C$TLI  # tucker lewis index
1- ((EFA_fit_6C$STATISTIC-EFA_fit_6C$dof)/
      (EFA_fit_6C$null.chisq-EFA_fit_6C$null.dof))  #CFI
```

